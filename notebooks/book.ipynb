{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6752c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d8cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ef6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c0f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingestion\n",
    "DATASET_LOC = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\n",
    "df=pd.read_csv(DATASET_LOC)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07bc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f118119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b536d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "# test_size=0.2\n",
    "# train_df,val_df=train_test_split(df,test_size=test_size,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e72df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbdb86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df.tag.value_counts() * int((1-test_size)/test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f89083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns;sns.set_theme()\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecea302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_tags=Counter(df.tag)\n",
    "# all_tags.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8eadb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags, tag_counts = zip(*all_tags.most_common())\n",
    "# plt.figure(figsize=(10, 3))\n",
    "# ax = sns.barplot(x=list(tags), y=list(tag_counts))\n",
    "# ax.set_xticklabels(tags, rotation=0, fontsize=8)\n",
    "# plt.title(\"Tag distribution\", fontsize=14)\n",
    "# plt.ylabel(\"# of projects\", fontsize=12)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ec740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent tokens for each tag\n",
    "# tag = \"natural-language-processing\"\n",
    "# plt.figure(figsize=(10, 3))\n",
    "# subset = df[df.tag == tag]\n",
    "# text = subset.title.values\n",
    "# cloud = WordCloud(\n",
    "#     stopwords=STOPWORDS,\n",
    "#     background_color=\"black\",\n",
    "#     collocations=False,\n",
    "#     width=500,\n",
    "#     height=300,\n",
    "# ).generate(\" \".join(text))\n",
    "# plt.axis(\"off\")\n",
    "# plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45456b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "STOPWORDS=stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5189370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, stopwords=STOPWORDS):\n",
    "    \"\"\"Clean raw text string.\"\"\"\n",
    "    # Lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove stopwords\n",
    "    pattern = re.compile(r\"\\b(\" + r\"|\".join(stopwords) + r\")\\b\\s*\")\n",
    "    text = pattern.sub(\"\", text)\n",
    "\n",
    "    # Spacing and filters\n",
    "    text = re.sub(\n",
    "        r\"([!\\\"'#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text\n",
    "    )  # add spacing\n",
    "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)  # remove non alphanumeric chars\n",
    "    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n",
    "    text = text.strip()  # strip white space at the ends\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  #  remove links\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9eacfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decode(indices,index_to_class):\n",
    "#     return [index_to_class[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b5a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_to_class={v:k for k,v in class_to_index.items()}\n",
    "# decode(df.head()[\"tag\"].values,index_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c0451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        \"allenai/scibert_scivocab_uncased\", return_dict=False\n",
    "    )\n",
    "    encoded_inputs = tokenizer(batch[\"text\"].tolist(), return_tensors=\"np\", padding=\"longest\")\n",
    "    return dict(ids=encoded_inputs[\"input_ids\"], mask=encoded_inputs[\"attention_mask\"], targets=np.array(batch[\"tag\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478405a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df,class_to_index):\n",
    "    \"\"\"Preprocess the data.\"\"\"\n",
    "    df[\"text\"] = df.title + \" \" + df.description  # feature engineering\n",
    "    df[\"text\"] = df.text.apply(clean_text)  # clean text\n",
    "    df = df.drop(\n",
    "        columns=[\"id\", \"created_on\", \"title\", \"description\"], errors=\"ignore\"\n",
    "    )  # clean dataframe\n",
    "    df = df[[\"text\", \"tag\"]]  # rearrange columns\n",
    "    df[\"tag\"] = df[\"tag\"].map(class_to_index)  # label encoding\n",
    "    outputs = tokenize(df)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95bf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess(train_df,class_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ray.data.read_csv(DATASET_LOC)\n",
    "ds=ds.random_shuffle(seed=1234)\n",
    "ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f3b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data import Dataset\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5225505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_split(\n",
    "    ds: Dataset,\n",
    "    stratify: str,\n",
    "    test_size: float,\n",
    "    shuffle: bool = True,\n",
    "    seed: int = 1234,\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"Split a dataset into train and test splits with equal\n",
    "    amounts of data points from each class in the column we\n",
    "    want to stratify on.\n",
    "\n",
    "    Args:\n",
    "        ds (Dataset): Input dataset to split.\n",
    "        stratify (str): Name of column to split on.\n",
    "        test_size (float): Proportion of dataset to split for test set.\n",
    "        shuffle (bool, optional): whether to shuffle the dataset. Defaults to True.\n",
    "        seed (int, optional): seed for shuffling. Defaults to 1234.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dataset, Dataset]: the stratified train and test datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    def _add_split(\n",
    "        df: pd.DataFrame,\n",
    "    ) -> pd.DataFrame:  # pragma: no cover, used in parent function\n",
    "        \"\"\"Naively split a dataframe into train and test splits.\n",
    "        Add a column specifying whether it's the train or test split.\"\"\"\n",
    "        train, test = train_test_split(\n",
    "            df, test_size=test_size, shuffle=shuffle, random_state=seed\n",
    "        )\n",
    "        train[\"_split\"] = \"train\"\n",
    "        test[\"_split\"] = \"test\"\n",
    "        return pd.concat([train, test])\n",
    "\n",
    "    def _filter_split(\n",
    "        df: pd.DataFrame, split: str\n",
    "    ) -> pd.DataFrame:  # pragma: no cover, used in parent function\n",
    "        \"\"\"Filter by data points that match the split column's value\n",
    "        and return the dataframe with the _split column dropped.\"\"\"\n",
    "        return df[df[\"_split\"] == split].drop(\"_split\", axis=1)\n",
    "\n",
    "    # Train, test split with stratify\n",
    "    grouped = ds.groupby(stratify).map_groups(\n",
    "        _add_split, batch_format=\"pandas\"\n",
    "    )  # group by each unique value in the column we want to stratify on\n",
    "    train_ds = grouped.map_batches(\n",
    "        _filter_split, fn_kwargs={\"split\": \"train\"}, batch_format=\"pandas\"\n",
    "    )  # combine\n",
    "    test_ds = grouped.map_batches(\n",
    "        _filter_split, fn_kwargs={\"split\": \"test\"}, batch_format=\"pandas\"\n",
    "    )  # combine\n",
    "\n",
    "    # Shuffle each split (required)\n",
    "    train_ds = train_ds.random_shuffle(seed=seed)\n",
    "    test_ds = test_ds.random_shuffle(seed=seed)\n",
    "\n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9409e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size=0.2\n",
    "train_ds,val_ds = stratify_split(ds,stratify=\"tag\",test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479eeb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags=train_ds.unique(column=\"tag\")\n",
    "# print(tags)\n",
    "class_to_index={tag:i for i,tag in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff947206",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ds=train_ds.map_batches(\n",
    "    preprocess,\n",
    "    fn_kwargs={\"class_to_index\":class_to_index},\n",
    "    batch_format=\"pandas\"\n",
    ")\n",
    "simple_ds.show(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
